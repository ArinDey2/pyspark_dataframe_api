{"paragraphs":[{"text":"%pyspark\n# Exploring Spark DataFrame API using PySpark","user":"ctv","dateUpdated":"2019-05-24T09:37:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558676479228_-1595999046","id":"20190524-054119_2140029985","dateCreated":"2019-05-24T05:41:19+0000","dateStarted":"2019-05-24T09:37:55+0000","dateFinished":"2019-05-24T09:38:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:356"},{"text":"%pyspark\r\n\r\n# Create DataFrame with few employee records\r\n\r\nemployee_list = []\r\n\r\nemployee1 = {\"emp_id\": 1, \"emp_name\": \"Arun\", \"emp_salary\": 50000, \"emp_city\": \"Chennai\", \"dept_id\": 1}\r\nemployee_list.append(employee1)\r\nemployee2 = {\"emp_id\": 2, \"emp_name\": \"Arvind\", \"emp_salary\": 80000, \"emp_city\": \"Chennai\", \"dept_id\": 2}\r\nemployee_list.append(employee2)\r\nemployee3 = {\"emp_id\": 3, \"emp_name\": \"Deepak\", \"emp_salary\": 30000, \"emp_city\": \"Chennai\", \"dept_id\": 1}\r\nemployee_list.append(employee3)\r\nemployee4 = {\"emp_id\": 4, \"emp_name\": \"Karthik\", \"emp_salary\": 45000, \"emp_city\": \"Hyderabad\", \"dept_id\": 3}\r\nemployee_list.append(employee4)\r\nemployee5 = {\"emp_id\": 5, \"emp_name\": \"Manoj\", \"emp_salary\": 75000, \"emp_city\": \"Bangalore\", \"dept_id\": 2}\r\nemployee_list.append(employee5)\r\nemployee6 = {\"emp_id\": 6, \"emp_name\": \"Prabhu\", \"emp_salary\": 25000, \"emp_city\": \"Chennai\", \"dept_id\": 4}\r\nemployee_list.append(employee6)\r\nemployee7 = {\"emp_id\": 7, \"emp_name\": \"Balu\", \"emp_salary\": 60000, \"emp_city\": \"Hyderabad\", \"dept_id\": 1}\r\nemployee_list.append(employee7)\r\nemployee8 = {\"emp_id\": 8, \"emp_name\": \"Arjun\", \"emp_salary\": 70000, \"emp_city\": \"Hyderabad\", \"dept_id\": 4}\r\nemployee_list.append(employee8)\r\nemployee9 = {\"emp_id\": 9, \"emp_name\": \"Ajay\", \"emp_salary\": 65000, \"emp_city\": \"Bangalore\", \"dept_id\": 6}\r\nemployee_list.append(employee9)\r\nemployee10 = {\"emp_id\": 10, \"emp_name\": \"Vasanth\", \"emp_salary\": 90000, \"emp_city\": \"Bangalore\", \"dept_id\": 7}\r\nemployee_list.append(employee10)\r\n\r\nemployee_df = spark.createDataFrame(employee_list)\r\n\r\nemployee_df.show(10, False)","user":"ctv","dateUpdated":"2019-05-24T10:45:57+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---------+------+--------+----------+\n|dept_id|emp_city |emp_id|emp_name|emp_salary|\n+-------+---------+------+--------+----------+\n|1      |Chennai  |1     |Arun    |50000     |\n|2      |Chennai  |2     |Arvind  |80000     |\n|1      |Chennai  |3     |Deepak  |30000     |\n|3      |Hyderabad|4     |Karthik |45000     |\n|2      |Bangalore|5     |Manoj   |75000     |\n|4      |Chennai  |6     |Prabhu  |25000     |\n|1      |Hyderabad|7     |Balu    |60000     |\n|4      |Hyderabad|8     |Arjun   |70000     |\n|6      |Bangalore|9     |Ajay    |65000     |\n|7      |Bangalore|10    |Vasanth |90000     |\n+-------+---------+------+--------+----------+\n\n"}]},"apps":[],"jobName":"paragraph_1558676550720_812459707","id":"20190524-054230_942128179","dateCreated":"2019-05-24T05:42:30+0000","dateStarted":"2019-05-24T10:45:57+0000","dateFinished":"2019-05-24T10:45:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:357","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=64"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\ndepartment_list = []\n\ndepartment1 = {\"dept_id\": 1, \"dept_name\": \"Finance\"}\ndepartment_list.append(department1)\ndepartment2 = {\"dept_id\": 2, \"dept_name\": \"Human Resource\"}\ndepartment_list.append(department2)\ndepartment3 = {\"dept_id\": 3, \"dept_name\": \"Administration\"}\ndepartment_list.append(department3)\ndepartment4 = {\"dept_id\": 4, \"dept_name\": \"Information Technology\"}\ndepartment_list.append(department4)\ndepartment5 = {\"dept_id\": 5, \"dept_name\": \"Training and Development\"}\ndepartment_list.append(department5)\n\ndepartment_df = spark.createDataFrame(department_list)\n\ndepartment_df.show(2, False)","user":"ctv","dateUpdated":"2019-05-24T09:45:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------+\n|dept_id|dept_name     |\n+-------+--------------+\n|1      |Finance       |\n|2      |Human Resource|\n+-------+--------------+\nonly showing top 2 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558676666920_1627524220","id":"20190524-054426_1462287434","dateCreated":"2019-05-24T05:44:26+0000","dateStarted":"2019-05-24T09:45:27+0000","dateFinished":"2019-05-24T09:45:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:358","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=41"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Select operation on DataFrame\n\ncolumns_list = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_city\", \"dept_id\"]\nemployee_df.select(columns_list).show(5, False)\n\n","user":"ctv","dateUpdated":"2019-05-24T10:46:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+----------+---------+-------+\n|emp_id|emp_name|emp_salary|emp_city |dept_id|\n+------+--------+----------+---------+-------+\n|1     |Arun    |50000     |Chennai  |1      |\n|2     |Arvind  |80000     |Chennai  |2      |\n|3     |Deepak  |30000     |Chennai  |1      |\n|4     |Karthik |45000     |Hyderabad|3      |\n|5     |Manoj   |75000     |Bangalore|2      |\n+------+--------+----------+---------+-------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558676721352_-425743409","id":"20190524-054521_1294838986","dateCreated":"2019-05-24T05:45:21+0000","dateStarted":"2019-05-24T10:46:46+0000","dateFinished":"2019-05-24T10:46:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:359","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=65"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Create DataFrame by reading CSV files from HDFS\ntransaction_detail_df = spark.read \\\n.option(\"header\", True) \\\n.option(\"inferSchema\", True) \\\n.csv(\"hdfs://instance-1:8020/user/hadoop/data/ecommerce_data/transactional_detail/2019-05-20/*\")\n\ntransaction_detail_df.show(2, False)\n\ntransaction_detail_df.printSchema()\n","user":"ctv","dateUpdated":"2019-05-24T10:49:52+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+\n|transaction_id|transaction_card_type|transaction_ecommerce_website_name|transaction_product_name|transaction_amount|transaction_datetime|transaction_city_name|transaction_country_name|\n+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+\n|2001          |MasterCard           |www.ebay.com                      |Wrist Band              |463.39            |2019-05-20 16:41:01 |Bangalore            |India                   |\n|2002          |Maestro              |www.amazon.com                    |TV                      |535.01            |2019-05-20 17:23:07 |Singapore            |Singapore               |\n+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+\nonly showing top 2 rows\n\nroot\n |-- transaction_id: integer (nullable = true)\n |-- transaction_card_type: string (nullable = true)\n |-- transaction_ecommerce_website_name: string (nullable = true)\n |-- transaction_product_name: string (nullable = true)\n |-- transaction_amount: double (nullable = true)\n |-- transaction_datetime: timestamp (nullable = true)\n |-- transaction_city_name: string (nullable = true)\n |-- transaction_country_name: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1558677384122_1991071398","id":"20190524-055624_797597358","dateCreated":"2019-05-24T05:56:24+0000","dateStarted":"2019-05-24T10:49:52+0000","dateFinished":"2019-05-24T10:49:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:360","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=69","http://10.142.0.2:4040/jobs/job?id=70","http://10.142.0.2:4040/jobs/job?id=71"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Finding DataFrame records/rows count\n\nrecords_count = transaction_detail_df.count()\nprint(records_count)","user":"ctv","dateUpdated":"2019-05-24T10:50:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1000\n"}]},"apps":[],"jobName":"paragraph_1558678804660_-371438518","id":"20190524-062004_1293314709","dateCreated":"2019-05-24T06:20:04+0000","dateStarted":"2019-05-24T10:50:36+0000","dateFinished":"2019-05-24T10:50:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:361","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=72"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Filter operation on DataFrame - based transaction_country_name\n\ntransaction_detail_country_df = transaction_detail_df.filter(\"transaction_country_name = 'India'\")\n\ntransaction_detail_country_df.show(5, False)","user":"ctv","dateUpdated":"2019-05-24T10:51:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+\n|transaction_id|transaction_card_type|transaction_ecommerce_website_name|transaction_product_name|transaction_amount|transaction_datetime|transaction_city_name|transaction_country_name|\n+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+\n|2001          |MasterCard           |www.ebay.com                      |Wrist Band              |463.39            |2019-05-20 16:41:01 |Bangalore            |India                   |\n|2006          |Visa                 |www.amazon.com                    |LAN Cable               |168.07            |2019-05-20 07:22:31 |Pune                 |India                   |\n|2007          |MasterCard           |www.flipkart.com                  |Pen Drive               |388.57            |2019-05-20 15:55:10 |Hyderabad            |India                   |\n|2008          |Visa                 |www.snapdeal.com                  |TV Stand                |33.77             |2019-05-20 15:50:39 |Bangalore            |India                   |\n|2009          |Visa                 |www.flipkart.com                  |Pen Drive               |337.77            |2019-05-20 05:29:02 |Hyderabad            |India                   |\n+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558678921668_-174993692","id":"20190524-062201_1607382753","dateCreated":"2019-05-24T06:22:01+0000","dateStarted":"2019-05-24T10:51:26+0000","dateFinished":"2019-05-24T10:51:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:362","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=73"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Adding new column to the DataFrame\n\nfrom pyspark.sql.functions import *\ntransaction_detail_country_df_upper = transaction_detail_country_df.withColumn(\"transaction_card_type_upper\", upper(transaction_detail_country_df.transaction_card_type))\n\ntransaction_detail_country_df_upper.show(5, False)","user":"ctv","dateUpdated":"2019-05-24T10:53:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+---------------------------+\n|transaction_id|transaction_card_type|transaction_ecommerce_website_name|transaction_product_name|transaction_amount|transaction_datetime|transaction_city_name|transaction_country_name|transaction_card_type_upper|\n+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+---------------------------+\n|2001          |MasterCard           |www.ebay.com                      |Wrist Band              |463.39            |2019-05-20 16:41:01 |Bangalore            |India                   |MASTERCARD                 |\n|2006          |Visa                 |www.amazon.com                    |LAN Cable               |168.07            |2019-05-20 07:22:31 |Pune                 |India                   |VISA                       |\n|2007          |MasterCard           |www.flipkart.com                  |Pen Drive               |388.57            |2019-05-20 15:55:10 |Hyderabad            |India                   |MASTERCARD                 |\n|2008          |Visa                 |www.snapdeal.com                  |TV Stand                |33.77             |2019-05-20 15:50:39 |Bangalore            |India                   |VISA                       |\n|2009          |Visa                 |www.flipkart.com                  |Pen Drive               |337.77            |2019-05-20 05:29:02 |Hyderabad            |India                   |VISA                       |\n+--------------+---------------------+----------------------------------+------------------------+------------------+--------------------+---------------------+------------------------+---------------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558679472172_-831298618","id":"20190524-063112_1486935845","dateCreated":"2019-05-24T06:31:12+0000","dateStarted":"2019-05-24T10:53:14+0000","dateFinished":"2019-05-24T10:53:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:363","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=74"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Aggregate operation on the DataFrame\n\ntransaction_detail_df_agg = transaction_detail_df.groupBy('transaction_country_name').agg({'transaction_amount':'sum'})\n\ntransaction_detail_df_agg.show(5, False)","user":"ctv","dateUpdated":"2019-05-24T10:54:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------+-----------------------+\n|transaction_country_name|sum(transaction_amount)|\n+------------------------+-----------------------+\n|Singapore               |19314.12               |\n|France                  |24538.02               |\n|India                   |115379.87999999999     |\n|United States           |18968.839999999997     |\n|Italy                   |21431.859999999997     |\n+------------------------+-----------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558680310128_525545137","id":"20190524-064510_707807919","dateCreated":"2019-05-24T06:45:10+0000","dateStarted":"2019-05-24T10:54:42+0000","dateFinished":"2019-05-24T10:54:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:364","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=75","http://10.142.0.2:4040/jobs/job?id=76","http://10.142.0.2:4040/jobs/job?id=77","http://10.142.0.2:4040/jobs/job?id=78"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\ntransaction_detail_df_agg = transaction_detail_df.groupBy('transaction_country_name').agg({'*': 'count', 'transaction_amount':'sum'})\n\ntransaction_detail_df_agg.show(5, False)","user":"ctv","dateUpdated":"2019-05-24T10:55:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------+-----------------------+--------+\n|transaction_country_name|sum(transaction_amount)|count(1)|\n+------------------------+-----------------------+--------+\n|Singapore               |19314.12               |67      |\n|France                  |24538.02               |85      |\n|India                   |115379.87999999999     |405     |\n|United States           |18968.839999999997     |72      |\n|Italy                   |21431.859999999997     |71      |\n+------------------------+-----------------------+--------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558682752635_510189978","id":"20190524-072552_2002391004","dateCreated":"2019-05-24T07:25:52+0000","dateStarted":"2019-05-24T10:55:36+0000","dateFinished":"2019-05-24T10:55:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:365","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=79","http://10.142.0.2:4040/jobs/job?id=80","http://10.142.0.2:4040/jobs/job?id=81","http://10.142.0.2:4040/jobs/job?id=82"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\ntransaction_detail_df_agg = transaction_detail_df.groupBy('transaction_country_name').agg({'*': 'count', 'transaction_amount':'sum'}).toDF('transaction_country_name', 'transaction_amount_sum', 'records_count')\n\ntransaction_detail_df_agg.show(5, False)\n\ntransaction_detail_df_agg.createTempView(\"transaction_dtl_agg_tbl\")\n","user":"ctv","dateUpdated":"2019-05-24T10:56:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"+------------------------+----------------------+-------------+\n|transaction_country_name|transaction_amount_sum|records_count|\n+------------------------+----------------------+-------------+\n|Singapore               |19314.12              |67           |\n|France                  |24538.02              |85           |\n|India                   |115379.87999999999    |405          |\n|United States           |18968.839999999997    |72           |\n|Italy                   |21431.859999999997    |71           |\n+------------------------+----------------------+-------------+\nonly showing top 5 rows\n\n"},{"type":"TEXT","data":"Fail to execute line 5: transaction_detail_df_agg.createTempView(\"transaction_dtl_agg_tbl\")\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6017204396968504703.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 5, in <module>\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/dataframe.py\", line 153, in createTempView\n    self._jdf.createTempView(name)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\", line 71, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\nAnalysisException: u\"Temporary table 'transaction_dtl_agg_tbl' already exists;\"\n"}]},"apps":[],"jobName":"paragraph_1558683440618_1147078765","id":"20190524-073720_291874792","dateCreated":"2019-05-24T07:37:20+0000","dateStarted":"2019-05-24T10:56:35+0000","dateFinished":"2019-05-24T10:56:35+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:366","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=83","http://10.142.0.2:4040/jobs/job?id=84","http://10.142.0.2:4040/jobs/job?id=85","http://10.142.0.2:4040/jobs/job?id=86"],"interpreterSettingId":"spark"}}},{"text":"%sql\n\nselect * from transaction_dtl_agg_tbl\n","user":"ctv","dateUpdated":"2019-05-24T10:57:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"transaction_country_name":"string","transaction_amount_sum":"string","records_count":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"transaction_country_name","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"transaction_amount_sum","index":1,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"transaction_country_name\ttransaction_amount_sum\trecords_count\nSingapore\t19314.12\t67\nFrance\t24538.02\t85\nIndia\t115379.87999999999\t405\nUnited States\t18968.839999999997\t72\nItaly\t21431.859999999997\t71\nThailand\t22228.190000000006\t80\nIsrael\t24123.12\t82\nInida\t20059.62\t75\nUnited Kingdom\t18124.579999999998\t63\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1558683937976_734895481","id":"20190524-074537_2100044989","dateCreated":"2019-05-24T07:45:37+0000","dateStarted":"2019-05-24T10:56:59+0000","dateFinished":"2019-05-24T10:56:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:367","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=87","http://10.142.0.2:4040/jobs/job?id=88","http://10.142.0.2:4040/jobs/job?id=89","http://10.142.0.2:4040/jobs/job?id=90","http://10.142.0.2:4040/jobs/job?id=91"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\ndepartment_df.show()\n","user":"ctv","dateUpdated":"2019-05-24T10:57:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+\n|dept_id|           dept_name|\n+-------+--------------------+\n|      1|             Finance|\n|      2|      Human Resource|\n|      3|      Administration|\n|      4|Information Techn...|\n|      5|Training and Deve...|\n+-------+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1558684022604_-1625740362","id":"20190524-074702_2073130755","dateCreated":"2019-05-24T07:47:02+0000","dateStarted":"2019-05-24T10:57:58+0000","dateFinished":"2019-05-24T10:57:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:368","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=92"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Union operation on DataFrame\n\ndepartment_df1 = department_df\n\ndepartment_df_union = department_df.union(department_df1)\n\ndepartment_df_union.show(100, False)","user":"ctv","dateUpdated":"2019-05-24T09:38:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------------------------+\n|dept_id|dept_name               |\n+-------+------------------------+\n|1      |Finance                 |\n|2      |Human Resource          |\n|3      |Administration          |\n|4      |Information Technology  |\n|5      |Training and Development|\n|1      |Finance                 |\n|2      |Human Resource          |\n|3      |Administration          |\n|4      |Information Technology  |\n|5      |Training and Development|\n+-------+------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1558684147820_1556504995","id":"20190524-074907_319772269","dateCreated":"2019-05-24T07:49:07+0000","dateStarted":"2019-05-24T09:38:29+0000","dateFinished":"2019-05-24T09:38:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:369","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=27","http://10.142.0.2:4040/jobs/job?id=28"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\ndepartment_df_union_distinct = department_df_union.distinct()\n\ndepartment_df_union_distinct.show(100, False)","user":"ctv","dateUpdated":"2019-05-24T09:38:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------------------------+\n|dept_id|dept_name               |\n+-------+------------------------+\n|3      |Administration          |\n|1      |Finance                 |\n|2      |Human Resource          |\n|5      |Training and Development|\n|4      |Information Technology  |\n+-------+------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1558684245041_-710788071","id":"20190524-075045_742388844","dateCreated":"2019-05-24T07:50:45+0000","dateStarted":"2019-05-24T09:38:29+0000","dateFinished":"2019-05-24T09:38:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:370","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=29","http://10.142.0.2:4040/jobs/job?id=30","http://10.142.0.2:4040/jobs/job?id=31","http://10.142.0.2:4040/jobs/job?id=32","http://10.142.0.2:4040/jobs/job?id=33"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Employees data\n\nemployee_df.select(columns_list).show(100, False)\n\ndepartment_df.show(100, False)","user":"ctv","dateUpdated":"2019-05-24T09:46:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558684328525_-658659112","id":"20190524-075208_278616741","dateCreated":"2019-05-24T07:52:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:371","dateFinished":"2019-05-24T09:46:40+0000","dateStarted":"2019-05-24T09:46:40+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+----------+---------+-------+\n|emp_id|emp_name|emp_salary|emp_city |dept_id|\n+------+--------+----------+---------+-------+\n|1     |Arun    |50000     |Chennai  |1      |\n|2     |Arvind  |80000     |Chennai  |2      |\n|3     |Deepak  |30000     |Chennai  |1      |\n|4     |Karthik |45000     |Hyderabad|3      |\n|5     |Manoj   |75000     |Bangalore|2      |\n|6     |Prabhu  |25000     |Chennai  |4      |\n|7     |Balu    |60000     |Hyderabad|1      |\n|8     |Arjun   |70000     |Hyderabad|4      |\n|9     |Ajay    |65000     |Bangalore|6      |\n|10    |Vasanth |90000     |Bangalore|7      |\n+------+--------+----------+---------+-------+\n\n+-------+------------------------+\n|dept_id|dept_name               |\n+-------+------------------------+\n|1      |Finance                 |\n|2      |Human Resource          |\n|3      |Administration          |\n|4      |Information Technology  |\n|5      |Training and Development|\n+-------+------------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=44","http://10.142.0.2:4040/jobs/job?id=45"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\n# Join operation on DataFrame\n# Inner Join\n\njoin_columns_list = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_city\", \"dept_id\", \"dept_name\"]\n\nemployee_df.join(department_df, \"dept_id\").select(join_columns_list).show(100, False)","user":"ctv","dateUpdated":"2019-05-24T11:01:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558690994890_-243764156","id":"20190524-094314_1222494001","dateCreated":"2019-05-24T09:43:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2766","dateFinished":"2019-05-24T11:01:45+0000","dateStarted":"2019-05-24T11:01:44+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+----------+---------+-------+----------------------+\n|emp_id|emp_name|emp_salary|emp_city |dept_id|dept_name             |\n+------+--------+----------+---------+-------+----------------------+\n|1     |Arun    |50000     |Chennai  |1      |Finance               |\n|3     |Deepak  |30000     |Chennai  |1      |Finance               |\n|7     |Balu    |60000     |Hyderabad|1      |Finance               |\n|4     |Karthik |45000     |Hyderabad|3      |Administration        |\n|2     |Arvind  |80000     |Chennai  |2      |Human Resource        |\n|5     |Manoj   |75000     |Bangalore|2      |Human Resource        |\n|6     |Prabhu  |25000     |Chennai  |4      |Information Technology|\n|8     |Arjun   |70000     |Hyderabad|4      |Information Technology|\n+------+--------+----------+---------+-------+----------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=93","http://10.142.0.2:4040/jobs/job?id=94","http://10.142.0.2:4040/jobs/job?id=95","http://10.142.0.2:4040/jobs/job?id=96","http://10.142.0.2:4040/jobs/job?id=97"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nemployee_df.join(department_df, \"dept_id\").select(join_columns_list).orderBy(\"emp_id\").show(100, False)","user":"ctv","dateUpdated":"2019-05-24T11:02:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558690711331_32866296","id":"20190524-093831_524629749","dateCreated":"2019-05-24T09:38:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2637","dateFinished":"2019-05-24T11:02:22+0000","dateStarted":"2019-05-24T11:02:21+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+----------+---------+-------+----------------------+\n|emp_id|emp_name|emp_salary|emp_city |dept_id|dept_name             |\n+------+--------+----------+---------+-------+----------------------+\n|1     |Arun    |50000     |Chennai  |1      |Finance               |\n|2     |Arvind  |80000     |Chennai  |2      |Human Resource        |\n|3     |Deepak  |30000     |Chennai  |1      |Finance               |\n|4     |Karthik |45000     |Hyderabad|3      |Administration        |\n|5     |Manoj   |75000     |Bangalore|2      |Human Resource        |\n|6     |Prabhu  |25000     |Chennai  |4      |Information Technology|\n|7     |Balu    |60000     |Hyderabad|1      |Finance               |\n|8     |Arjun   |70000     |Hyderabad|4      |Information Technology|\n+------+--------+----------+---------+-------+----------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=98"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nemployee_df.join(department_df, employee_df.dept_id == department_df.dept_id).select(employee_df[\"*\"], department_df[\"*\"]).orderBy(\"emp_id\").show(100, False)","user":"ctv","dateUpdated":"2019-05-24T11:03:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558691421078_-1898882521","id":"20190524-095021_55354176","dateCreated":"2019-05-24T09:50:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3154","dateFinished":"2019-05-24T11:03:24+0000","dateStarted":"2019-05-24T11:03:23+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---------+------+--------+----------+-------+----------------------+\n|dept_id|emp_city |emp_id|emp_name|emp_salary|dept_id|dept_name             |\n+-------+---------+------+--------+----------+-------+----------------------+\n|1      |Chennai  |1     |Arun    |50000     |1      |Finance               |\n|2      |Chennai  |2     |Arvind  |80000     |2      |Human Resource        |\n|1      |Chennai  |3     |Deepak  |30000     |1      |Finance               |\n|3      |Hyderabad|4     |Karthik |45000     |3      |Administration        |\n|2      |Bangalore|5     |Manoj   |75000     |2      |Human Resource        |\n|4      |Chennai  |6     |Prabhu  |25000     |4      |Information Technology|\n|1      |Hyderabad|7     |Balu    |60000     |1      |Finance               |\n|4      |Hyderabad|8     |Arjun   |70000     |4      |Information Technology|\n+-------+---------+------+--------+----------+-------+----------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=99"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nemployee_df.join(department_df, employee_df.dept_id == department_df.dept_id, \"inner\").select(employee_df[\"*\"], department_df[\"*\"]).orderBy(\"emp_id\").show(100, False)","user":"ctv","dateUpdated":"2019-05-24T11:03:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558692019862_-1832360988","id":"20190524-100019_1277292817","dateCreated":"2019-05-24T10:00:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3272","dateFinished":"2019-05-24T11:03:48+0000","dateStarted":"2019-05-24T11:03:47+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---------+------+--------+----------+-------+----------------------+\n|dept_id|emp_city |emp_id|emp_name|emp_salary|dept_id|dept_name             |\n+-------+---------+------+--------+----------+-------+----------------------+\n|1      |Chennai  |1     |Arun    |50000     |1      |Finance               |\n|2      |Chennai  |2     |Arvind  |80000     |2      |Human Resource        |\n|1      |Chennai  |3     |Deepak  |30000     |1      |Finance               |\n|3      |Hyderabad|4     |Karthik |45000     |3      |Administration        |\n|2      |Bangalore|5     |Manoj   |75000     |2      |Human Resource        |\n|4      |Chennai  |6     |Prabhu  |25000     |4      |Information Technology|\n|1      |Hyderabad|7     |Balu    |60000     |1      |Finance               |\n|4      |Hyderabad|8     |Arjun   |70000     |4      |Information Technology|\n+-------+---------+------+--------+----------+-------+----------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=101"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nemployee_df.join(department_df, employee_df.dept_id == department_df.dept_id, \"left_outer\").select(employee_df[\"*\"], department_df[\"*\"]).orderBy(\"emp_id\").show(100, False)","user":"ctv","dateUpdated":"2019-05-24T10:13:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558692790066_-595327764","id":"20190524-101310_544362948","dateCreated":"2019-05-24T10:13:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3488","dateFinished":"2019-05-24T10:13:54+0000","dateStarted":"2019-05-24T10:13:53+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---------+------+--------+----------+-------+----------------------+\n|dept_id|emp_city |emp_id|emp_name|emp_salary|dept_id|dept_name             |\n+-------+---------+------+--------+----------+-------+----------------------+\n|1      |Chennai  |1     |Arun    |50000     |1      |Finance               |\n|2      |Chennai  |2     |Arvind  |80000     |2      |Human Resource        |\n|1      |Chennai  |3     |Deepak  |30000     |1      |Finance               |\n|3      |Hyderabad|4     |Karthik |45000     |3      |Administration        |\n|2      |Bangalore|5     |Manoj   |75000     |2      |Human Resource        |\n|4      |Chennai  |6     |Prabhu  |25000     |4      |Information Technology|\n|1      |Hyderabad|7     |Balu    |60000     |1      |Finance               |\n|4      |Hyderabad|8     |Arjun   |70000     |4      |Information Technology|\n|6      |Bangalore|9     |Ajay    |65000     |null   |null                  |\n|7      |Bangalore|10    |Vasanth |90000     |null   |null                  |\n+-------+---------+------+--------+----------+-------+----------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=60"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nemployee_df.join(department_df, employee_df.dept_id == department_df.dept_id, \"right_outer\").select(employee_df[\"*\"], department_df[\"*\"]).orderBy(\"emp_id\").show(100, False)","user":"ctv","dateUpdated":"2019-05-24T10:14:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558692833043_-241763270","id":"20190524-101353_1167371433","dateCreated":"2019-05-24T10:13:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3606","dateFinished":"2019-05-24T10:14:28+0000","dateStarted":"2019-05-24T10:14:27+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---------+------+--------+----------+-------+------------------------+\n|dept_id|emp_city |emp_id|emp_name|emp_salary|dept_id|dept_name               |\n+-------+---------+------+--------+----------+-------+------------------------+\n|null   |null     |null  |null    |null      |5      |Training and Development|\n|1      |Chennai  |1     |Arun    |50000     |1      |Finance                 |\n|2      |Chennai  |2     |Arvind  |80000     |2      |Human Resource          |\n|1      |Chennai  |3     |Deepak  |30000     |1      |Finance                 |\n|3      |Hyderabad|4     |Karthik |45000     |3      |Administration          |\n|2      |Bangalore|5     |Manoj   |75000     |2      |Human Resource          |\n|4      |Chennai  |6     |Prabhu  |25000     |4      |Information Technology  |\n|1      |Hyderabad|7     |Balu    |60000     |1      |Finance                 |\n|4      |Hyderabad|8     |Arjun   |70000     |4      |Information Technology  |\n+-------+---------+------+--------+----------+-------+------------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=61"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n\nemployee_df.join(department_df, employee_df.dept_id == department_df.dept_id, \"full_outer\").select(employee_df[\"*\"], department_df[\"*\"]).orderBy(\"emp_id\").show(100, False)\n","user":"ctv","dateUpdated":"2019-05-24T11:05:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558692857272_406623431","id":"20190524-101417_240005790","dateCreated":"2019-05-24T10:14:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3727","dateFinished":"2019-05-24T11:05:37+0000","dateStarted":"2019-05-24T11:05:36+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---------+------+--------+----------+-------+------------------------+\n|dept_id|emp_city |emp_id|emp_name|emp_salary|dept_id|dept_name               |\n+-------+---------+------+--------+----------+-------+------------------------+\n|null   |null     |null  |null    |null      |5      |Training and Development|\n|1      |Chennai  |1     |Arun    |50000     |1      |Finance                 |\n|2      |Chennai  |2     |Arvind  |80000     |2      |Human Resource          |\n|1      |Chennai  |3     |Deepak  |30000     |1      |Finance                 |\n|3      |Hyderabad|4     |Karthik |45000     |3      |Administration          |\n|2      |Bangalore|5     |Manoj   |75000     |2      |Human Resource          |\n|4      |Chennai  |6     |Prabhu  |25000     |4      |Information Technology  |\n|1      |Hyderabad|7     |Balu    |60000     |1      |Finance                 |\n|4      |Hyderabad|8     |Arjun   |70000     |4      |Information Technology  |\n|6      |Bangalore|9     |Ajay    |65000     |null   |null                    |\n|7      |Bangalore|10    |Vasanth |90000     |null   |null                    |\n+-------+---------+------+--------+----------+-------+------------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://10.142.0.2:4040/jobs/job?id=102"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n","user":"ctv","dateUpdated":"2019-05-24T10:15:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558692951680_743631799","id":"20190524-101551_1696208715","dateCreated":"2019-05-24T10:15:51+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3864"}],"name":"python_examples/Exploring Spark DataFrame API using PySpark","id":"2EECCWNBR","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}